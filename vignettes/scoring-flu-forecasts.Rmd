---
title: "Scoring Flusight submissions using scoringutils"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Scoring Flusight submissions using scoringutils}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "svg",
  fig.ext = "svg",
  fig.height = 16,
  fig.width = 8
)
```

```{r setup, message = FALSE}
library(forecasttools)
library(scoringutils)
library(dplyr)
library(ggplot2)
library(knitr)
```

In this vignette, we use `forecasttools` to capture the current state of the FluSight forecast hub (see [here](https://github.com/cdcepi/FluSight-forecast-hub)), and then score the forecasts according to a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule). We do the scoring with [`scoringutils`](https://github.com/epiforecasts/scoringutils).

### Generating a table of forecasts against truth data.

First, we create a table of quantile forecasts formatted to work with `scoringutils` functions using `hub_to_scorable_quantiles()`. Generally, we expect users to use `hub_to_scorable_quantiles()` with a local path to the forecast repository which updates from GitHub by default. In this case, we download a copy of the Hub from GitHub.

```{r}
hub_url <- "https://github.com/cdcepi/FluSight-forecast-hub"
hub_path <- fs::path(withr::local_tempdir(), "flusight-hub")
download_hub(
  hub_url = hub_url,
  hub_path = hub_path,
  force = TRUE
)
```

For reproducibility, in this vignette we will examine the Hub as of a specific [git commit](https://github.com/git-guides/git-commit), [`b311e92`](https://github.com/cdcepi/FluSight-forecast-hub/commit/b311e92).
```{r, include = FALSE}
## use a specific commit to future-proof the vignette
gert::git_branch_create(
  branch = "flu-scoring-vignette-temp",
  repo = hub_path,
  force = FALSE,
  checkout = TRUE,
  ref = "b311e92"
)
```

The schema for both forecasts and ["target data"](https://docs.hubverse.io/en/latest/user-guide/target-data.html) is standardized across Hubverse hubs. There are two [standard formats for Hubverse target data](https://docs.hubverse.io/en/latest/user-guide/target-data.html#uses-of-target-time-series-data-and-oracle-output): "timeseries" format and "oracle output" format. To work with `hub_to_scorable_quantiles()`, a forecast hub must provide "oracle output" format target data.

As of [`b311e92`](https://github.com/cdcepi/FluSight-forecast-hub/commit/b311e92), the FluSight Hub accepted quantile forecasts for a single "target" quantity: epiweekly incident influenza hospital admissions. It provided a oracle output target data in a file called [`oracle-output.csv`](https://github.com/cdcepi/FluSight-forecast-hub/blob/b311e926ca3995d0c16f5a3d626665959d52f387/target-data/oracle-output.csv).

`hub_to_scorable_quantiles()` just needs to be pointed at the local copy of the Hub, and it will extract available quantile forecasts (via `hubData::connect_hub()`), combine them with the appropriate associated target data fetched from the "oracle output" (via `hubData::connect_target_oracle_output()`), and produce a `scoringutils`-ready table via `scoringutils::as_forecast_quantile()`:

```{r}
forecast_and_target <- hub_to_scorable_quantiles(hub_path)

head(forecast_and_target)
```

Note that while the `hubData` package identifies individual models with a `model_id` column, `hub_to_scorable_quantiles()` renames this to the `scoringutils` standard column name for models: `model`.

There were `r forecast_and_target$model |> unique() |> length()` different models that had submitted quantile forecasts to FluSight as of [`b311e92`](https://github.com/cdcepi/FluSight-forecast-hub/commit/b311e92).

```{r}
unique(forecast_and_target$model)
```

There are `r forecast_and_target$location |> unique() |> na.omit() |> length()` locations, either states or territories, for which there are available forecasts. They are stored as two-digit codes, but can re-code them as the more familiar USPS-style two-letter abbreviations via `us_loc_code_to_abbr()`:

```{r}
unique(forecast_and_target$location)

forecast_and_target <- forecast_and_target |>
  mutate(location = us_loc_code_to_abbr(location))

unique(forecast_and_target$location)
```

### Tabular scoring of forecasts

`scoringutils` provides various forecast evaluation metrics including interval scores, skill relative to a chosen baseline, and coverage at different prediction quantiles. Here we show the metrics for US overall forecasts by model for all forecast dates so far, rounding to two significant figures


```{r}
chosen_location <- "US"

forecast_and_target |>
  filter(location == !!chosen_location) |>
  score() |>
  summarise_scores(
    by = "model",
    relative_skill = TRUE,
    baseline = "FluSight-ensemble"
  ) |>
  summarise_scores(
    by = "model",
    fun = signif,
    digits = 2
  ) |>
  kable()
```
