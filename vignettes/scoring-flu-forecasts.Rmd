---
title: "Scoring Flusight submissions using scoringutils"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Scoring Flusight submissions using scoringutils}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  dev = "svg",
  fig.ext = "svg",
  fig.height = 16,
  fig.width = 8
)
```

```{r setup, message = FALSE}
library(forecasttools)
library(scoringutils)
library(dplyr)
library(ggplot2)
library(knitr)
```

In this vignette, we use `forecasttools` to capture the current state of the FluSight forecast hub (see [here](https://github.com/cdcepi/FluSight-forecast-hub)), and then score the forecasts according to a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule). We do the scoring with [`scoringutils`](https://github.com/epiforecasts/scoringutils).

### Generating a table of forecasts against truth data.

First, we create a table of forecast predictions formatted to work with `scoringutils` functions using `hub_to_scorable_quantiles()`. Generally, we expect users to use `hub_to_scorable_quantiles()` with a local path to the forecast repository which updates from GitHub by default. In this case, we download the hub first.

```{r}
hub_url <- "https://github.com/cdcepi/FluSight-forecast-hub"
hub_path <- fs::path(withr::local_tempdir(), "flusight-hub")
download_hub(
  hub_url = hub_url,
  hub_path = hub_path,
  force = TRUE
)
```

```{r, include = FALSE}
## use a specific commit to future-proof the vignette
gert::git_branch_create(
  branch = "flu-scoring-vignette-temp",
  repo = hub_path,
  force = FALSE,
  checkout = TRUE,
  ref = "6ae6919"
)
```

```{r}
forecast_and_target <- hub_to_scorable_quantiles(hub_path)
```

There are `r forecast_and_target$model_id |> unique() |> length()` different models that have been submitted to FluSight.

```{r}
unique(forecast_and_target$model_id)
```

There are `r forecast_and_target$location |> unique() |> na.omit() |> length()` locations, either states or territories, for which there are available fforecasts. They are stored as two-digit codes, but can re-code them as the more familiar USPS-style two-letter abbreviations via `us_loc_code_to_abbr()`:

```{r}
unique(forecast_and_target$location)

forecast_and_target <- forecast_and_target |>
  mutate(location = us_loc_code_to_abbr(location))

unique(forecast_and_target$location)
```

### Tabular scoring of forecasts

`scoringutils` provides various forecast evaluation metrics including interval scores, skill relative to a chosen baseline, and coverage at different prediction quantiles. Here we show the metrics for US overall forecasts by model for all forecast dates so far, rounding to two significant figures


```{r}
chosen_location <- "US"

forecast_and_target |>
  filter(location == !!chosen_location) |>
  score() |>
  summarise_scores(
    by = "model_id",
    relative_skill = TRUE,
    baseline = "FluSight-ensemble"
  ) |>
  summarise_scores(
    by = "model_id",
    fun = signif,
    digits = 2
  ) |>
  kable()
```
